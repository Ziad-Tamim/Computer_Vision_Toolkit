{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4747c07e-e105-4654-a93a-65dd8470e78b",
   "metadata": {
    "id": "4747c07e-e105-4654-a93a-65dd8470e78b"
   },
   "source": [
    "# Coursework 1\n",
    "\n",
    "Author: Ziad Tamim\n",
    "\n",
    "Date: 06/10/2023\n",
    "\n",
    "Description: This file include coursework 1 Answers This includes:\n",
    "\n",
    "    1- Image Transformations\n",
    "    2- Convolution\n",
    "    3- Video Segmentation\n",
    "    4- Texture Classification\n",
    "    5- Object Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1nx4GB7q65D8",
   "metadata": {
    "id": "1nx4GB7q65D8"
   },
   "source": [
    "Dataset files path for fast and easy access to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vI48m4L26346",
   "metadata": {
    "id": "vI48m4L26346"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/ziadt/Desktop/Projects/MSc - AI modules assignments/Introduction to Computer Vision - SEM1/Coursework 1/Submission/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Name in Aerial image\n",
    "name_img = 'Dataset/Name in Arial.png'\n",
    "\n",
    "# Dataset A path\n",
    "face_im1 = 'Dataset/face-1.jpg'\n",
    "face_im2 = 'Dataset/face-2.jpg'\n",
    "face_im3 = 'Dataset/face-3.jpg'\n",
    "car_im1 = 'Dataset/car-1.jpg'\n",
    "car_im2 = 'Dataset/car-2.jpg'\n",
    "car_im3 = 'Dataset/car-3.jpg'\n",
    "\n",
    "test_face = 'Dataset/Test_face_image.jpg'\n",
    "\n",
    "# Video files path\n",
    "# dataset_b = 'Dataset/DatasetB.avi'\n",
    "# dataset_c ='Dataset/DatasetC.avi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b9e6c-c530-452e-b485-370b97717a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qsH4XqpBxYE7",
   "metadata": {
    "id": "qsH4XqpBxYE7"
   },
   "source": [
    "# 1) Transformations.\n",
    "Rotation, translation and skew are useful operations for matching, tracking, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce7854-dbe2-4530-9d8f-823480f04e40",
   "metadata": {
    "id": "efce7854-dbe2-4530-9d8f-823480f04e40"
   },
   "source": [
    "1.a)  Write a function that takes as input an image I, rotates it by an angle θ1 and horizontally skews it by\n",
    "an angle, θ2. Write the matrix formulation for image rotation R(.) and skewing S(.). Define all the\n",
    "variables. Note that the origin of the coordinate system of the programming environment you use\n",
    "might be different from the one shown in the lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3dc02-6a3a-48b8-b3a9-0406bd7850e4",
   "metadata": {
    "id": "ada3dc02-6a3a-48b8-b3a9-0406bd7850e4"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rotate_and_skew(image_path, theta1, theta2):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    rows, cols, _ = image.shape\n",
    "\n",
    "    # Compute the rotation matrix\n",
    "    R = np.array([[np.cos(theta1), -np.sin(theta1)],\n",
    "                  [np.sin(theta1), np.cos(theta1)]])\n",
    "\n",
    "    # Compute the skew matrix (for horizontal skewing)\n",
    "    S = np.array([[1, np.tan(theta2)],\n",
    "                  [0, 1]])\n",
    "\n",
    "    # Computing the transformation matrix T = R * S\n",
    "    T = np.dot(R, S)\n",
    "\n",
    "    # Initialize an empty image for the output\n",
    "    output_image = np.zeros_like(image)\n",
    "\n",
    "    # Compute the inverse of T for backward mapping\n",
    "    T_inv = np.linalg.inv(T)\n",
    "\n",
    "    # Iterate over the destination image (output_image)\n",
    "    for y in range(rows):\n",
    "        for x in range(cols):\n",
    "            # Backward mapping: Find the source position for each destination pixel\n",
    "            src_pos = np.dot(T_inv, np.array([x - cols/2, y - rows/2]))  # Subtract half of dimensions for center-based rotation\n",
    "            src_x, src_y = src_pos[0] + cols/2, src_pos[1] + rows/2  # Add them back after transformation\n",
    "\n",
    "            # Bilinear interpolation could be added here for better results, but for simplicity, we'll use nearest-neighbor:\n",
    "            if 0 <= src_x < cols and 0 <= src_y < rows:\n",
    "                output_image[y, x] = image[int(src_y), int(src_x)]\n",
    "\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dcfcee-6cee-45cf-9bc6-ba04b6b60bbf",
   "metadata": {
    "editable": true,
    "id": "13dcfcee-6cee-45cf-9bc6-ba04b6b60bbf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1.b) Create an image that contains your name written in Arial, point 72, capital letters. Rotate clockwise\n",
    "the image you created by 30, 60, 120 and -50 degrees. Skew the same image by 10, 40 and 60\n",
    "degrees. Complete the process so that all the pixels have a value. Discuss in the report the\n",
    "advantages and disadvantages of different approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bafe40-999b-4a77-9ffd-331b174a7a21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e86b57-d6ad-44e0-8487-9d00d149edd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "29e86b57-d6ad-44e0-8487-9d00d149edd6",
    "outputId": "8613234a-0f12-4b26-f8ae-5d09e81fc872"
   },
   "outputs": [],
   "source": [
    "# Chose Roration and Translation\n",
    "theta_1 = 20  # Rotation\n",
    "theta_2 = 30 # Skew\n",
    "\n",
    "# Use the function\n",
    "image_path = name_img\n",
    "result_image = rotate_and_skew(image_path, np.radians(theta_1), np.radians(theta_2))\n",
    "\n",
    "# Plotting the result image\n",
    "plt.imshow(result_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1db50-1e90-4218-90fd-01dd8b4ec8c8",
   "metadata": {
    "id": "15f1db50-1e90-4218-90fd-01dd8b4ec8c8"
   },
   "source": [
    "1.c) Analyse the results when you change the order of the two operators: R(S(I)) and S(R(I)).\n",
    "\n",
    "1.c.i) Rotate the image by θ1 = 20 clockwise and then skew the result by θ2 = 50.\n",
    "\n",
    "1.c.ii) Skew the image by θ2 = 50 and then rotate the result by θ1 = 20 clockwise.\n",
    "\n",
    "Are the results of (i) and (ii) the same? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RSqVUv-OYkgJ",
   "metadata": {
    "id": "RSqVUv-OYkgJ"
   },
   "source": [
    "ans)\n",
    " When changing the order of the rotation (R) and the skew (S) matrix operations on the image the results of the output differ, as seen in the figures above. This result comes from computing the transformation matrix T = R * S when the order of the rotation and skewing matrix change the results differ, figure of code is seen below. This is because it involves taking the row elements of the first matrix and column elements of the second matrix, multiplying them together, and summing them up. As a result, the order and placement of the matrices heavily influence how the multiplication turns out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594b19b-690b-4eff-b8be-028196a7ac0c",
   "metadata": {
    "id": "8594b19b-690b-4eff-b8be-028196a7ac0c"
   },
   "source": [
    "# 2) Convolution.\n",
    "Convolution provides a way of multiplying two arrays to produce a third array. Depending on the designed\n",
    "filter and the intended effect, the kernel can be a matrix of dimensions, for example, 3x3, 5x5 or 7x7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256d232-d534-459e-bae0-f381ada52a20",
   "metadata": {
    "id": "c256d232-d534-459e-bae0-f381ada52a20"
   },
   "source": [
    "a) Code a function that takes an input image, performs convolution with a given kernel, and returns the\n",
    "resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b05e53-8fc0-4f71-b0b9-261c1fdf7128",
   "metadata": {
    "id": "d6b05e53-8fc0-4f71-b0b9-261c1fdf7128"
   },
   "outputs": [],
   "source": [
    "def convolve2D(image, kernel):\n",
    "    # Retrieve the dimensions of the image and the kernel\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    # Calculate the amount of padding needed\n",
    "    pad_height = kernel_height // 2\n",
    "    pad_width = kernel_width // 2\n",
    "\n",
    "    # Calculate new image dimensions\n",
    "    new_height = image_height + (2 * pad_height)\n",
    "    new_width = image_width + (2 * pad_width)\n",
    "\n",
    "    # Create a new image with zeros for padding\n",
    "    padded_image = np.zeros((new_height, new_width))\n",
    "\n",
    "    # Copy the contents of the image into the center of this new padded image\n",
    "    padded_image[pad_height:pad_height + image_height, pad_width:pad_width + image_width] = image\n",
    "\n",
    "    # Prepare an output image of the same size as the input\n",
    "    output = np.zeros_like(image)\n",
    "\n",
    "    # Perform convolution\n",
    "    for y in range(image_height):\n",
    "        for x in range(image_width):\n",
    "            # Element-wise multiplication of the kernel and the corresponding image area and sum\n",
    "            output[y, x] = np.sum(kernel * padded_image[y:y + kernel_height, x:x + kernel_width])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v5zxSUxupFMJ",
   "metadata": {
    "id": "v5zxSUxupFMJ"
   },
   "source": [
    "b) Design a convolution kernel that computes, for each pixel, the average intensity value in a 3x3 region.\n",
    "Use this kernel and the filtering function above, and save the resulting image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b144cc-03da-4e59-af0a-a1428562c366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "59b144cc-03da-4e59-af0a-a1428562c366",
    "outputId": "ce43c7e2-5cb6-42b5-dbfb-c7f310cbe82d"
   },
   "outputs": [],
   "source": [
    "# Average filter\n",
    "kernel_ave = np.array([[1, 1, 1],\n",
    "                    [1, 1, 1],\n",
    "                    [1, 1, 1]]) /9\n",
    "\n",
    "# Load image\n",
    "image_path = car_im1\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Appling convolution\n",
    "convolved_image = convolve2D(image, kernel_ave)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Original image ploted\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Avrage filter on image plotted\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(convolved_image, cmap='gray')\n",
    "plt.title('Avrage filter (Gray Scale)')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olGhW3TfKoAt",
   "metadata": {
    "id": "olGhW3TfKoAt"
   },
   "source": [
    "Use the kernels provided below, apply the filtering function and save the resulting images. Comment\n",
    "on the effect of each kernel.\n",
    "\n",
    "kernel A\n",
    "\n",
    "1 2 1\n",
    "\n",
    "2 4 2\n",
    "\n",
    "1 2 1\n",
    "\n",
    "\n",
    "kernel B\n",
    "\n",
    "0 1 0\n",
    "\n",
    "1 -4 1\n",
    "\n",
    "0 1 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I3Svp_3ovI6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "I3Svp_3ovI6c",
    "outputId": "237e860e-4d48-4e29-c11c-be6d7255ef23"
   },
   "outputs": [],
   "source": [
    "kernel_A = np.array([[1, 2, 1],\n",
    "                    [2, 4, 2],\n",
    "                    [1, 2, 1]]) /16\n",
    "\n",
    "kernel_B = np.array([[0, 1, 0],\n",
    "                    [1, -4, 1],\n",
    "                    [0, 1, 0]])\n",
    "\n",
    "\n",
    "# Kernel A Applied\n",
    "convolved_image_A = convolve2D(image, kernel_A)\n",
    "\n",
    "# Kernel B Applied\n",
    "convolved_image_B = convolve2D(image, kernel_B)\n",
    "\n",
    "# Adjust figsize here\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plotting convolution with given Kernal A\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(convolved_image_A, cmap='gray')\n",
    "plt.title('Kernal A')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plotting convolution with given Kernal A\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(convolved_image_B, cmap='gray')\n",
    "plt.title('Kernal B')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZH1pzyP6vIYh",
   "metadata": {
    "id": "ZH1pzyP6vIYh"
   },
   "source": [
    "d) Use the filtering function for the following filtering operations: (i) A followed by A; (ii) A followed by B;\n",
    "(iii) B followed by A. Comment the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NRrOSqa50TBp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "NRrOSqa50TBp",
    "outputId": "4d620b6f-5e4e-4206-d10f-1548330e5873"
   },
   "outputs": [],
   "source": [
    "# Assuming image is your input image\n",
    "# Applying A followed by A\n",
    "image_AA = convolve2D(convolve2D(image, kernel_A), kernel_A)\n",
    "\n",
    "# Applying A followed by B\n",
    "image_AB = convolve2D(convolve2D(image, kernel_A), kernel_B)\n",
    "\n",
    "# Applying B followed by A\n",
    "image_BA = convolve2D(convolve2D(image, kernel_B), kernel_A)\n",
    "\n",
    "# Adjust figsize here\n",
    "plt.figure(figsize=(20, 10))  # Width = 10 inches, Height = 8 inches, adjust as needed\n",
    "\n",
    "# Plotting convolution with given Kernal A\n",
    "plt.subplot(3,1,1)\n",
    "plt.imshow(image_AA, cmap='gray')\n",
    "plt.title('Kernal A A')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plotting convolution with given Kernal A\n",
    "plt.subplot(3,1,2)\n",
    "plt.imshow(image_AB, cmap='gray')\n",
    "plt.title('Kernal A B')\n",
    "plt.axis('off')\n",
    "\n",
    "# Plotting convolution with given Kernal A\n",
    "plt.subplot(3,1,3)\n",
    "plt.imshow(image_BA, cmap='gray')\n",
    "plt.title('Kernal B A')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tn4-y0ghyju6",
   "metadata": {
    "id": "tn4-y0ghyju6"
   },
   "source": [
    "Comments:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0AXOzF0Xo6_W",
   "metadata": {
    "id": "0AXOzF0Xo6_W"
   },
   "source": [
    "# 3) Video Segmentation.\n",
    "A colour histogram h(.) can be generated by counting how many times each colour occurs in an image.\n",
    "Histogram intersection can be used to match a pair of histograms. Given a pair of histograms, e.g., of an\n",
    "input image I and a model M, each containing n bins, the intersection of the histograms is defined as\n",
    "∑ min[h(Ij), h(Mj)]\n",
    "n\n",
    "j=1\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTwg4TdpRUp",
   "metadata": {
    "id": "unTwg4TdpRUp"
   },
   "source": [
    "a)  Write a histogram function that returns the colour histogram of an input image. Visualize the histogram\n",
    "and save the corresponding figure. For a given video sequence, use the above function to construct\n",
    "the histogram of each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LBdzMi5wpVMx",
   "metadata": {
    "id": "LBdzMi5wpVMx"
   },
   "outputs": [],
   "source": [
    "# calculate_histogram function description this function calculates the bins\n",
    "# for each color RGB image and returnes the histogram for R, G and B colour\n",
    "def calculate_histogram(image, bins):\n",
    "    # Initialize histograms for R, G, B\n",
    "    histogram_r = np.zeros(bins)\n",
    "    histogram_g = np.zeros(bins)\n",
    "    histogram_b = np.zeros(bins)\n",
    "\n",
    "    # Calculate bin width\n",
    "    bin_width = 256 / bins\n",
    "\n",
    "    # Iterate through each pixel and update histograms\n",
    "    for row in image:\n",
    "        for pixel in row:\n",
    "            r, g, b = pixel\n",
    "            histogram_r[int(r // bin_width)] += 1\n",
    "            histogram_g[int(g // bin_width)] += 1\n",
    "            histogram_b[int(b // bin_width)] += 1\n",
    "\n",
    "    return histogram_r, histogram_g, histogram_b\n",
    "\n",
    "# visualize_and_save_histogram function takes in the Histogram of each image plots them in a bar chart then saves the histogram as .png\n",
    "def visualize_and_save_histogram(histogram_r, histogram_g, histogram_b, bins):\n",
    "    # Define bin edges\n",
    "    bin_edges = np.arange(bins + 1) * (256 / bins) - 0.5\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(bin_edges[:-1], histogram_r, width=256/bins, color='red', alpha=0.7, label='Red Channel')\n",
    "    plt.bar(bin_edges[:-1], histogram_g, width=256/bins, color='green', alpha=0.7, label='Green Channel')\n",
    "    plt.bar(bin_edges[:-1], histogram_b, width=256/bins, color='blue', alpha=0.7, label='Blue Channel')\n",
    "    plt.legend()\n",
    "    plt.title(\"Color Histogram\")\n",
    "    plt.xlabel(\"Color value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlim([-0.5, 255.5])\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "    # Save plot\n",
    "    # plt.savefig(\"color_histogram.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NPXwehZ7wCoN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NPXwehZ7wCoN",
    "outputId": "c8608c58-2cd2-4b29-9ffc-c4e60797c07a"
   },
   "outputs": [],
   "source": [
    "## Test histogram function and visualization function on video sequencs\n",
    "Bin_num = 12 # Set bin number\n",
    "\n",
    "def frames_hist(video):\n",
    "    count = 0\n",
    "    success = True\n",
    "    while count < 347:  # To view all frames remove count < 50 to  while success.\n",
    "        success,frame = video.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Change frame\n",
    "        plt.imshow(frame)\n",
    "        plt.title('Frame number: {}'.format(count))\n",
    "        plt.axis('off')\n",
    "        histogram_r, histogram_g, histogram_b = calculate_histogram(frame, Bin_num) # calculate bins for each R G B color\n",
    "        visualize_and_save_histogram(histogram_r, histogram_g, histogram_b, Bin_num) # Visualize the histogram\n",
    "        count += 1\n",
    "        print()\n",
    "    return count\n",
    "\n",
    "\n",
    "video = cv2.VideoCapture(dataset_b)\n",
    "frame_count = frames_hist(video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1z8trZ1JHm2",
   "metadata": {
    "id": "a1z8trZ1JHm2"
   },
   "source": [
    "b) Write a function that returns the value of the intersection of a pair of histograms. For a given video\n",
    "sequence, use the histogram intersection function to calculate the intersection between consecutive\n",
    "frames (e.g. between It and It+1, between It+1 and It+2 and so on). Find how to normalize the\n",
    "intersection. Does that change the results? Plot the intersection values over time and the normalised\n",
    "intersection values, and save the corresponding figures. Show and comment the figures in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nga3bXHF7QsT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nga3bXHF7QsT",
    "outputId": "d26d5880-416f-4343-eeab-7874b19f816a"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the histogram intersection\n",
    "def histogram_intersection(h1, h2):\n",
    "    return np.minimum(h1, h2).sum()\n",
    "\n",
    "# Function to calculate the normalized histogram intersection\n",
    "def normalized_histogram_intersection(h1, h2):\n",
    "    return histogram_intersection(h1, h2) / h1.sum()\n",
    "\n",
    "# Function to process the video and calculate histograms for each frame\n",
    "def calculate_frame_histograms(video_path, bins, max_frames=None):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_histograms = []\n",
    "    success, frame = video.read()\n",
    "    frame_count = 0\n",
    "\n",
    "    while success and (max_frames is None or frame_count < max_frames):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        hist = calculate_histogram(frame, bins)\n",
    "        frame_histograms.append(hist)\n",
    "        success, frame = video.read()\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frame_histograms\n",
    "\n",
    "# Function to plot and save histogram intersections\n",
    "def plot_and_save_histogram_intersections(frame_histograms, bins):\n",
    "    intersections = []\n",
    "    normalized_intersections = []\n",
    "    count = 0\n",
    "    # Calculate intersections\n",
    "    for i in range(len(frame_histograms) - 1):\n",
    "        h1 = np.hstack(frame_histograms[i])\n",
    "        h2 = np.hstack(frame_histograms[i + 1])\n",
    "        intersection = histogram_intersection(h1, h2)\n",
    "        print(\"The intersection result between frames {} and {} is {}\".format(count, count + 1, intersection))\n",
    "        normalized_intersection = normalized_histogram_intersection(h1, h2)\n",
    "        print(\"The normalized intersection result between frames {} and {} is {}\\n\".format(count, count + 1, normalized_intersection))\n",
    "        intersections.append(intersection)\n",
    "        normalized_intersections.append(normalized_intersection)\n",
    "        count += 1\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(intersections, label='Intersection')\n",
    "    plt.title('Histogram Intersections over Time')\n",
    "    plt.xlabel('Frame number')\n",
    "    plt.ylabel('Intersection value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(normalized_intersections, label='Normalized Intersection', color='red')\n",
    "    plt.title('Normalized Histogram Intersections over Time')\n",
    "    plt.xlabel('Frame number')\n",
    "    plt.ylabel('Normalized Intersection value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(\"histogram_intersections.png\")\n",
    "\n",
    "# Main routine\n",
    "video_path = dataset_b  # Replace with your video path\n",
    "bins = 16\n",
    "max_frames = None # You can set this to None to process the whole video\n",
    "\n",
    "# Calculate histograms for each frame\n",
    "frame_histograms = calculate_frame_histograms(video_path, bins, max_frames)\n",
    "\n",
    "# Plot and save histogram intersections\n",
    "plot_and_save_histogram_intersections(frame_histograms, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUyfi91kdiWO",
   "metadata": {
    "id": "MUyfi91kdiWO"
   },
   "source": [
    "c) Discuss in the report the following: What does the intersection value represent for a given input video?\n",
    "Can you use it to make a decision about scene changes? How robust to changes in the video is the\n",
    "histogram intersection? When does it fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmpMUc_CwLHy",
   "metadata": {
    "id": "kmpMUc_CwLHy"
   },
   "source": [
    "# 4) Texture Classification.\n",
    "The Local Binary Pattern (LBP) operator describes the surroundings of a pixel by generating a bit-code\n",
    "from the binary derivatives of a pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlzqsbexwgGV",
   "metadata": {
    "id": "jlzqsbexwgGV"
   },
   "source": [
    "a) Write a function that divides a greyscale image into equally sized non-overlapping windows and\n",
    "returns the feature descriptor for each window as distribution of LBP codes. For each pixel in the\n",
    "window, compare the pixel to each of its 8 neighbours. Convert the resulting bit-codes (base 2) to\n",
    "decimals (base 10 numbers) and compute their histogram over the window. Normalize the histogram\n",
    "(which is now a feature descriptor representing the window). Show in the report the resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ipGYDt7SlzLl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ipGYDt7SlzLl",
    "outputId": "343aeb36-42b3-4a92-dd8c-deea39bbedae",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute LBP\n",
    "def get_lbp(window):\n",
    "    center = window[1, 1]\n",
    "    top_left = window[0, 0] >= center\n",
    "    top_up = window[0, 1] >= center\n",
    "    top_right = window[0, 2] >= center\n",
    "    right = window[1, 2] >= center\n",
    "    bottom_right = window[2, 2] >= center\n",
    "    bottom_down = window[2, 1] >= center\n",
    "    bottom_left = window[2, 0] >= center\n",
    "    left = window[1, 0] >= center\n",
    "    # Convert the booleans to binary numbers, then to the corresponding LBP\n",
    "    return (top_left << 7) | (top_up << 6) | (top_right << 5) | (right << 4) | \\\n",
    "           (bottom_right << 3) | (bottom_down << 2) | (bottom_left << 1) | left\n",
    "\n",
    "def lbp_feature_descriptor(image, window_size):\n",
    "    # Divide the image into non-overlapping windows of specified size\n",
    "    h, w = image.shape\n",
    "    windows = []\n",
    "    lbp_images = []\n",
    "    histograms = []\n",
    "\n",
    "    # Define number of windows along x and y axes\n",
    "    num_windows_x = w // window_size\n",
    "    num_windows_y = h // window_size\n",
    "\n",
    "    for x in range(0, num_windows_x):\n",
    "        for y in range(0, num_windows_y):\n",
    "            # Extract the window from the image\n",
    "            window = image[y*window_size:(y+1)*window_size, x*window_size:(x+1)*window_size]\n",
    "            windows.append(window)\n",
    "\n",
    "            # Compute LBP for the window\n",
    "            lbp_window = np.zeros_like(window)\n",
    "            for i in range(1, window_size - 1):\n",
    "                for j in range(1, window_size - 1):\n",
    "                    lbp_window[i, j] = get_lbp(window[i-1:i+2, j-1:j+2])\n",
    "\n",
    "            lbp_images.append(lbp_window)\n",
    "\n",
    "            # Compute histogram of LBP values\n",
    "            hist, _ = np.histogram(lbp_window.ravel(), bins=np.arange(0, 256), density=True)\n",
    "            histograms.append(hist)\n",
    "\n",
    "    return windows, lbp_images, histograms\n",
    "\n",
    "# Gray scale image\n",
    "image = cv2.imread(car_im1, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Define window size\n",
    "window_size = 80\n",
    "\n",
    "# Get LBP feature descriptors\n",
    "windows, lbp_images, histograms = lbp_feature_descriptor(image, window_size)\n",
    "\n",
    "# Display window, lbp image and histograms \n",
    "indices_to_display = [0, len(windows)//2, len(windows)-1]  # non-consecutive indices\n",
    "for index in indices_to_display:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(windows[index], cmap='gray')\n",
    "    plt.title(f'Window {index+1}')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(lbp_images[index], cmap='gray')\n",
    "    plt.title(f'LBP Window {index+1}')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(np.arange(len(histograms[index])), histograms[index])\n",
    "    plt.title(f'Histogram {index+1}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NeP2a-sswnFF",
   "metadata": {
    "id": "NeP2a-sswnFF"
   },
   "source": [
    "b) Come up with a descriptor that represents the whole image as consisting of multiple windows. For\n",
    "example, you could combine several local descriptions into a global description by concatenation.\n",
    "Discuss in the report alternative approaches. Using the global descriptor you created, implement a\n",
    "classification process that separates the images in the dataset into two categories: face images and\n",
    "non-face images (for example, you could use histogram similarities). Comment the results in the\n",
    "report. Is the global descriptor able to represent whole images of different types (e.g. faces vs. cars)?\n",
    "Identify problems (if any), discuss them in the report and suggest possible solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgncbY4Bw5nY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "id": "HgncbY4Bw5nY",
    "outputId": "6daaf135-32a8-4a11-f756-a47fa2531f1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "def plot_global_descriptor(descriptor, title):\n",
    "    # Plot the global descriptor as a bar plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(descriptor)), descriptor, width=1)\n",
    "    plt.title(f'Global Descriptor for {title}')\n",
    "    plt.xlabel('LBP Code')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def compute_global_descriptor(image_path, window_size=50): # Change window size from here\n",
    "    # Read the grayscale image\n",
    "    image = imread(image_path, as_gray=True)\n",
    "\n",
    "    # Compute LBP for each window and get histograms\n",
    "    _, _, histograms = lbp_feature_descriptor(image, window_size)\n",
    "\n",
    "    # Concatenate all histograms to form a global descriptor\n",
    "    global_descriptor = np.concatenate(histograms)\n",
    "    return global_descriptor\n",
    "\n",
    "# Paths to the images\n",
    "face_image_path = face_im1 # image 1 path\n",
    "car_image_path = car_im1  # image 2 path\n",
    "\n",
    "# Compute global descriptors for the given images\n",
    "face_descriptor = compute_global_descriptor(face_image_path)\n",
    "car_descriptor = compute_global_descriptor(car_image_path)\n",
    "\n",
    "# Plot the global descriptors\n",
    "plot_global_descriptor(face_descriptor, 'Face Image')\n",
    "plot_global_descriptor(car_descriptor, 'Car Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c2819-edf3-4eb7-bd61-374eac19d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_distance(hist1, hist2):\n",
    "    # Calculate the Chi-Squared distance between two histograms.\n",
    "    return np.sum((hist1 - hist2) ** 2 / (hist1 + hist2 + 1e-10))\n",
    "\n",
    "def classify_image(test_descriptor, reference_descriptors, labels):\n",
    "    # Classify an image as face or non-face based on histogram similarity.\n",
    "    distances = [chi_squared_distance(test_descriptor, ref) for ref in reference_descriptors]\n",
    "    closest_index = np.argmin(distances)\n",
    "    return labels[closest_index]\n",
    "\n",
    "# faces and non-faces path\n",
    "face_images = [\n",
    "    face_im1,\n",
    "    face_im2,\n",
    "    face_im3\n",
    "]\n",
    "\n",
    "non_face_images = [\n",
    "    car_im1,\n",
    "    car_im2,\n",
    "    car_im3\n",
    "]\n",
    "\n",
    "# Compute global descriptors for the training dataset\n",
    "face_descriptors = [compute_global_descriptor(face_image) for face_image in face_images]  # List of face image paths\n",
    "non_face_descriptors = [compute_global_descriptor(non_face_image) for non_face_image in non_face_images]  # List of non-face image paths\n",
    "\n",
    "# Combine descriptors and labels\n",
    "all_descriptors = face_descriptors + non_face_descriptors\n",
    "labels = ['face'] * len(face_descriptors) + ['non-face'] * len(non_face_descriptors)\n",
    "\n",
    "# Example classification of a new image\n",
    "test_image_path = test_face  # Replace with test image car_im1 or test_face\n",
    "test_descriptor = compute_global_descriptor(test_image_path)\n",
    "\n",
    "classification = classify_image(test_descriptor, all_descriptors, labels)\n",
    "print(f\"The image is classified as: {classification}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wEw4rQqw-eP",
   "metadata": {
    "id": "2wEw4rQqw-eP"
   },
   "source": [
    "c) Decrease the window size and perform classification again. Comment the results in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749e7f2-704f-4d50-af22-bc68de2f2cc6",
   "metadata": {
    "id": "ZW9uUYjJw_H8"
   },
   "source": [
    "Comment in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ufKR1Lt4w_zR",
   "metadata": {
    "id": "ufKR1Lt4w_zR"
   },
   "source": [
    "d) Increase the window size and perform classification again. Comment the results in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82fd97-ec30-49f7-af50-5aa8acfbfe4c",
   "metadata": {
    "id": "v2XqwJOfxFBW"
   },
   "source": [
    "Comment in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JRjdp8oKxFpf",
   "metadata": {
    "id": "JRjdp8oKxFpf"
   },
   "source": [
    "e) Discuss how LBP can be used or modified for the analysis of dynamic textures in a video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5bbf4-1d4b-41f9-90fb-cc57958f562a",
   "metadata": {
    "id": "q1lh3cB4xI8H"
   },
   "source": [
    "Comment in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xv3Y50DTxhbQ",
   "metadata": {
    "id": "xv3Y50DTxhbQ"
   },
   "source": [
    "# 5) Object Counting.\n",
    "Moving objects captured by fixed cameras are the focus of several computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IjMAW-aPxtON",
   "metadata": {
    "id": "IjMAW-aPxtON"
   },
   "source": [
    "a) Write a function that performs pixel-by-pixel frame differencing using, as reference frame, the first\n",
    "frame of an image sequence. Apply a classification threshold and save the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74hwOsTccPkz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "74hwOsTccPkz",
    "outputId": "4823b2a4-c6d6-44bb-97c6-abb9c7556c82"
   },
   "outputs": [],
   "source": [
    "def frame_differencing(video_path, threshold_value, frame_indices):\n",
    "    \"\"\"\n",
    "    Performs frame differencing on a video sequence\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file\n",
    "    - threshold_value: Value for classification threshold.\n",
    "    - frame_indices: A list of frame indices to compare with the first frame\n",
    "\n",
    "    Output:\n",
    "    - Plots original frames, frame differencing results, and threshold results\n",
    "    - Saves results\n",
    "    \"\"\"\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, reference_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error reading the first frame.\")\n",
    "        return\n",
    "\n",
    "    reference_frame_gray = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Plot the reference frame\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.imshow(reference_frame_gray, cmap='gray')\n",
    "    plt.title('Reference Frame')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Go through the selected frames\n",
    "    for i, frame_index in enumerate(frame_indices, start=1):\n",
    "        # Set the video frame position\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "\n",
    "        # Read the selected frame\n",
    "        ret, selected_frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error reading frame at index {frame_index}.\")\n",
    "            continue\n",
    "\n",
    "        selected_frame_gray = cv2.cvtColor(selected_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference using numpy to avoid using cv2 in the main algorithm\n",
    "        frame_diff = np.abs(reference_frame_gray.astype(np.int16) - selected_frame_gray.astype(np.int16)).astype(np.uint8)\n",
    "\n",
    "        # Apply threshold using numpy to avoid using cv2 in the main algorithm\n",
    "        thresh_img = np.where(frame_diff > threshold_value, 255, 0).astype(np.uint8)\n",
    "\n",
    "        # Save results using plt.imsave to avoid using cv2\n",
    "        #plt.imsave(f'frame_diff_{frame_index}.png', frame_diff, cmap='gray')\n",
    "        #plt.imsave(f'thresh_{frame_index}.png', thresh_img, cmap='gray')\n",
    "\n",
    "        # Plot original frames\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(selected_frame_gray, cmap='gray')\n",
    "        plt.title(f'Selected Frame {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot frame differencing results\n",
    "        plt.subplot(3, 3, i+4)\n",
    "        plt.imshow(frame_diff, cmap='gray')\n",
    "        plt.title(f'Frame Differencing {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot threshold results\n",
    "        plt.subplot(3, 3, i+7)\n",
    "        plt.imshow(thresh_img, cmap='gray')\n",
    "        plt.title(f'Threshold Result {i}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "# Define video path and parameters\n",
    "video_path = dataset_c\n",
    "threshold_value = 50 # Set threshold value\n",
    "frame_indices = [10, 30] # Set the frame indices you want to compare\n",
    "\n",
    "# Run the updated frame differencing function\n",
    "frame_differencing(video_path, threshold_value, frame_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4emJlDLx6us",
   "metadata": {
    "id": "t4emJlDLx6us"
   },
   "source": [
    "b) Repeat the exercise using the previous frame as reference frame (use frame It-1 as reference frame\n",
    "for frame It, for each t). Comment the results in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FK380NqigXLw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "FK380NqigXLw",
    "outputId": "7eff75ff-75a6-49b3-a55f-5b19888ca243"
   },
   "outputs": [],
   "source": [
    "def frame_differencing_previous(video_path, threshold_value, frame_indices):\n",
    "    \"\"\"\n",
    "    Performs frame differencing using the previous frame as the reference frame.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: Path to the video file\n",
    "    - threshold_value: Value for classification threshold.\n",
    "    - frame_indices: A list of frame indices to compare with the previous frame\n",
    "\n",
    "    Output:\n",
    "    - Plots original frames, frame differencing results, and threshold results\n",
    "    - Saves results\n",
    "    \"\"\"\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "\n",
    "    # Go through the selected frames\n",
    "    for i, frame_index in enumerate(frame_indices):\n",
    "        # Read the previous frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index-1)\n",
    "        ret, prev_frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error reading frame at index {frame_index-1}.\")\n",
    "            continue\n",
    "\n",
    "        # Convert previous frame to grayscale\n",
    "        prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Read the current frame\n",
    "        ret, current_frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error reading frame at index {frame_index}.\")\n",
    "            continue\n",
    "\n",
    "        # Convert current frame to grayscale\n",
    "        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference using numpy\n",
    "        frame_diff = np.abs(prev_frame_gray.astype(np.int16) - current_frame_gray.astype(np.int16)).astype(np.uint8)\n",
    "\n",
    "        # Apply threshold using numpy\n",
    "        thresh_img = np.where(frame_diff > threshold_value, 255, 0).astype(np.uint8)\n",
    "\n",
    "        # Save results\n",
    "        #plt.imsave(f'frame_diff_{frame_index}.png', frame_diff, cmap='gray')\n",
    "        #plt.imsave(f'thresh_{frame_index}.png', thresh_img, cmap='gray')\n",
    "\n",
    "        # Plot original frame\n",
    "        axes[0, i].imshow(current_frame_gray, cmap='gray')\n",
    "        axes[0, i].set_title(f'Selected Frame {frame_index}')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # Plot frame differencing result\n",
    "        axes[1, i].imshow(frame_diff, cmap='gray')\n",
    "        axes[1, i].set_title(f'Frame Differencing {frame_index}')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        # Plot threshold result\n",
    "        axes[2, i].imshow(thresh_img, cmap='gray')\n",
    "        axes[2, i].set_title(f'Threshold Result {frame_index}')\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "# Define video path and parameters\n",
    "video_path = dataset_c  \n",
    "threshold_value = 50  # Set threshold value\n",
    "frame_indices = [5, 20]  # Set the frame indices\n",
    "\n",
    "\n",
    "frame_differencing_previous(video_path, threshold_value, frame_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tYvCivLSyGfQ",
   "metadata": {
    "id": "tYvCivLSyGfQ"
   },
   "source": [
    "c) Write a function that generates a reference frame (background) for the sequence using for example\n",
    "frame differencing and a weighted temporal averaging algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28tO80bIh7hK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "28tO80bIh7hK",
    "outputId": "6f44a176-9fef-4c0f-f1b1-1be9c87501ef"
   },
   "outputs": [],
   "source": [
    "def generate_background(video_path, alpha):\n",
    "    \"\"\"\n",
    "    Generates a reference background frame for a video sequence using weighted temporal averaging.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path\n",
    "    - alpha: Weight for the running average\n",
    "\n",
    "    Output:\n",
    "    - Plots the generated background\n",
    "    \"\"\"\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return None\n",
    "\n",
    "    # Read the first frame to initialize the background\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    # Initialize the background with the first frame\n",
    "    background = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "\n",
    "    # Process the video frames\n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "\n",
    "        # Update the background using a weighted average (manual implementation)\n",
    "        background = (1 - alpha) * background + alpha * gray_frame\n",
    "\n",
    "    # Convert the floating background to uint8 for displaying\n",
    "    background_uint8 = np.clip(background, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Plot the generated background\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(background_uint8, cmap='gray')\n",
    "    plt.title('Generated Background')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "    return background_uint8\n",
    "\n",
    "# Video path\n",
    "video_path = dataset_c\n",
    "alpha = 0.04  # Alpha for weighted average\n",
    "\n",
    "background = generate_background(video_path, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KtBU55XlyKaT",
   "metadata": {
    "id": "KtBU55XlyKaT"
   },
   "source": [
    "d) Write a function that counts the number of moving objects in each frame of a sequence. Generate a\n",
    "bar plot that visualizes the number of objects for each frame of the whole sequence. Discuss in the\n",
    "report the implemented solution, including advantages and disadvantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WMSQdkUIl-SU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "WMSQdkUIl-SU",
    "outputId": "947c49b5-e3e3-4a66-9e81-8b18deb1945c"
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def count_moving_objects(video_path, background, threshold_value, min_contour_area=250):\n",
    "    \"\"\"\n",
    "    Counts the number of moving objects in each frame of a video sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path\n",
    "    - background: The background frame for reference used in question 5 c\n",
    "    - threshold_value\n",
    "    - min_contour_area: Minimum area for a contour to be considered an object\n",
    "\n",
    "    Output:\n",
    "    - A bar plot of the number of moving objects per frame\n",
    "    \"\"\"\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    frame_counts = []\n",
    "    frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the absolute difference with the background\n",
    "        frame_diff = np.abs(gray_frame.astype(np.int16) - background.astype(np.int16)).astype(np.uint8)\n",
    "\n",
    "        # Apply threshold\n",
    "        thresh = np.where(frame_diff > threshold_value, 255, 0).astype(np.uint8)\n",
    "\n",
    "        # Apply morphological operations to reduce noise\n",
    "        kernel = np.ones((5,5),np.uint8)\n",
    "        erosion = ndimage.binary_erosion(thresh, structure=kernel).astype(np.uint8)\n",
    "        dilation = ndimage.binary_dilation(erosion, structure=kernel).astype(np.uint8)\n",
    "\n",
    "        # Find contours using connected components\n",
    "        labeled_array, num_features = ndimage.label(dilation)\n",
    "\n",
    "        # Measure the size of each contour and filter out very small contours\n",
    "        object_slices = ndimage.find_objects(labeled_array)\n",
    "        moving_objects = [s for s in object_slices if np.product(labeled_array[s].shape) > min_contour_area]\n",
    "\n",
    "        # Count the number of moving objects\n",
    "        object_count = len(moving_objects)\n",
    "        frame_counts.append(object_count)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "\n",
    "    # Plot the bar plot of moving objects per frame\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(frame_counts)), frame_counts)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Number of Moving cars')\n",
    "    plt.title('Number of Moving cars Per Frame')\n",
    "    plt.show()\n",
    "\n",
    "    return frame_counts\n",
    "\n",
    "# Define video path and threshold value\n",
    "video_path = dataset_c  # video path\n",
    "threshold_value = 30  # Threshold value\n",
    "\n",
    "# Execute function\n",
    "moving_objects_counts = count_moving_objects(video_path, background, threshold_value) # Background used from previous question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HsnV3-rXIKhy",
   "metadata": {
    "id": "HsnV3-rXIKhy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
